"""Application configuration with Pydantic Settings.

This module provides centralized configuration management using pydantic-settings.
Settings are loaded from environment variables and .env files.

Examples:
    >>> from app.config import settings
    >>> settings.PRIMARY_PROVIDER
    <ProviderType.GOOGLE: 'google'>

    >>> settings.get_provider_config()
    ProviderConfig(primary=<ProviderType.GOOGLE>, fallback=<ProviderType.OPENROUTER>, ...)

Tests:
    - tests/unit/test_config.py::test_settings_defaults
    - tests/unit/test_config.py::test_provider_detection
    - tests/unit/test_config.py::test_database_url_parsing
"""

from enum import Enum
from functools import lru_cache
from typing import Any

from pydantic import Field, field_validator, model_validator
from pydantic_settings import BaseSettings, SettingsConfigDict


class ProviderType(str, Enum):
    """Supported LLM providers."""

    GOOGLE = "google"
    OPENROUTER = "openrouter"


class ParallelismMode(str, Enum):
    """Parallelism mode for pipeline execution.

    - SEQUENTIAL: 1 call at a time (safest, for debugging or severe rate limits)
    - NORMAL: Tier-based default parallelism (1-3 concurrent)
    - AGGRESSIVE: Higher tier-based parallelism (2-5 concurrent)
    - MAX: Maximum safe parallelism (provider limit - 1, up to 8)
    """

    SEQUENTIAL = "sequential"
    NORMAL = "normal"
    AGGRESSIVE = "aggressive"
    MAX = "max"


class QualityPreset(str, Enum):
    """Quality preset for generation pipeline.

    - HD: Highest quality with Gemini 3 Pro + Google image generation
    - HYPER: Fastest speed with Gemini 2.0 Flash via OpenRouter
    - BALANCED: Default balance of quality and speed
    - GEMINI3: Latest Gemini 3 Flash Preview via OpenRouter (thinking model)
    """

    HD = "hd"
    HYPER = "hyper"
    BALANCED = "balanced"
    GEMINI3 = "gemini3"


class Environment(str, Enum):
    """Application environment."""

    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION = "production"


# =============================================================================
# VERIFIED MODELS - Only use models from this list
# =============================================================================
# These models have been tested and confirmed working.
# DO NOT use any model not in this list - it will fail silently or loudly.
#
# Last verified: 2026-02-07
# =============================================================================

class VerifiedModels:
    """Verified working models for each provider.

    IMPORTANT: Only models in these lists are guaranteed to work.
    Using unverified models will result in failures.
    """

    # Google Native API (via google-genai SDK)
    # These work with GOOGLE_API_KEY
    GOOGLE_TEXT = [
        "gemini-2.5-flash",           # Fast, reliable, supports thinking
        "gemini-2.0-flash",           # Older but stable
    ]

    GOOGLE_IMAGE = [
        "gemini-2.5-flash-image",       # Nano Banana - fast 1K image generation
        "gemini-3-pro-image-preview",   # Nano Banana Pro - 2K/4K, best quality
    ]

    # OpenRouter API (via openrouter.ai)
    # These work with OPENROUTER_API_KEY
    OPENROUTER_TEXT = [
        "google/gemini-2.0-flash-001",        # Fast, handles JSON well
        "google/gemini-2.0-flash-001:free",   # Free tier (rate limited)
        "google/gemini-3-flash-preview",      # Latest thinking model, agentic workflows
    ]

    # Fallback chains - ordered by preference
    # When a model fails, try the next one
    TEXT_FALLBACK_CHAIN = [
        "gemini-2.5-flash",                   # Primary: Google native
        "google/gemini-2.0-flash-001",        # Fallback: OpenRouter
    ]

    IMAGE_FALLBACK_CHAIN = [
        "gemini-2.5-flash-image",             # Primary: Nano Banana
    ]

    @classmethod
    def is_verified_text_model(cls, model: str) -> bool:
        """Check if a text model is verified."""
        return model in cls.GOOGLE_TEXT or model in cls.OPENROUTER_TEXT

    @classmethod
    def is_verified_image_model(cls, model: str) -> bool:
        """Check if an image model is verified."""
        return model in cls.GOOGLE_IMAGE

    @classmethod
    def get_safe_text_model(cls, provider: "ProviderType") -> str:
        """Get a guaranteed working text model for a provider."""
        if provider == ProviderType.GOOGLE:
            return cls.GOOGLE_TEXT[0]  # gemini-2.5-flash
        else:
            return cls.OPENROUTER_TEXT[0]  # google/gemini-2.0-flash-001

    @classmethod
    def get_safe_image_model(cls) -> str:
        """Get a guaranteed working image model."""
        return cls.GOOGLE_IMAGE[0]  # gemini-2.5-flash-image


# Quality Preset Configurations
# =============================================================================
# IMPORTANT: All presets MUST use models from VerifiedModels class above
# =============================================================================
PRESET_CONFIGS: dict[QualityPreset, dict[str, Any]] = {
    QualityPreset.HD: {
        "name": "HD Quality",
        "description": "Highest quality - Gemini 2.5 Flash + Nano Banana Pro (2K images)",
        # All models from VerifiedModels.GOOGLE_TEXT and GOOGLE_IMAGE
        "text_model": "gemini-2.5-flash",         # VerifiedModels.GOOGLE_TEXT[0]
        "judge_model": "gemini-2.5-flash",        # VerifiedModels.GOOGLE_TEXT[0]
        "image_model": "gemini-3-pro-image-preview",  # Nano Banana Pro - 2K/4K support
        "image_provider": ProviderType.GOOGLE,
        "text_provider": ProviderType.GOOGLE,
        "max_tokens": 8192,
        "thinking_level": "high",  # Extended thinking for better quality
        "image_size": "2K",        # Nano Banana Pro supports 1K, 2K, 4K
    },
    QualityPreset.HYPER: {
        "name": "Hyper Speed",
        "description": "Fastest generation - Gemini 2.0 Flash via OpenRouter",
        # All models from VerifiedModels.OPENROUTER_TEXT and GOOGLE_IMAGE
        "text_model": "google/gemini-2.0-flash-001",  # VerifiedModels.OPENROUTER_TEXT[0]
        "judge_model": "google/gemini-2.0-flash-001", # VerifiedModels.OPENROUTER_TEXT[0]
        "image_model": "gemini-2.5-flash-image",      # VerifiedModels.GOOGLE_IMAGE[0]
        "image_provider": ProviderType.GOOGLE,
        "text_provider": ProviderType.OPENROUTER,
        "max_tokens": 1024,
        "thinking_level": None,
        "image_supported": True,
    },
    QualityPreset.BALANCED: {
        "name": "Balanced",
        "description": "Balance of quality and speed - Gemini 2.5 Flash",
        # All models from VerifiedModels.GOOGLE_TEXT and GOOGLE_IMAGE
        "text_model": "gemini-2.5-flash",       # VerifiedModels.GOOGLE_TEXT[0]
        "judge_model": "gemini-2.5-flash",      # VerifiedModels.GOOGLE_TEXT[0]
        "image_model": "gemini-2.5-flash-image", # VerifiedModels.GOOGLE_IMAGE[0]
        "image_provider": ProviderType.GOOGLE,
        "text_provider": ProviderType.GOOGLE,
        "max_tokens": 2048,
        "thinking_level": "medium",
    },
    QualityPreset.GEMINI3: {
        "name": "Gemini 3 Flash",
        "description": "Latest Gemini 3 Flash Preview - thinking model via OpenRouter",
        # Uses latest Gemini 3 Flash Preview for text, Google native for images
        "text_model": "google/gemini-3-flash-preview",  # VerifiedModels.OPENROUTER_TEXT[2]
        "judge_model": "google/gemini-3-flash-preview", # VerifiedModels.OPENROUTER_TEXT[2]
        "image_model": "gemini-2.5-flash-image",        # VerifiedModels.GOOGLE_IMAGE[0]
        "image_provider": ProviderType.GOOGLE,
        "text_provider": ProviderType.OPENROUTER,
        "max_tokens": 4096,
        "thinking_level": "medium",  # Gemini 3 supports configurable thinking
        "image_supported": True,
    },
}


# Parallelism Configuration
# Maps presets to their parallelism mode
PRESET_PARALLELISM: dict[QualityPreset, ParallelismMode] = {
    QualityPreset.HD: ParallelismMode.NORMAL,       # Quality focus, standard parallelism
    QualityPreset.BALANCED: ParallelismMode.NORMAL, # Default behavior
    QualityPreset.HYPER: ParallelismMode.MAX,       # Speed focus, maximum parallelism
    QualityPreset.GEMINI3: ParallelismMode.AGGRESSIVE,  # Thinking model, moderate parallelism
}

# Provider rate limits (requests per minute and safe concurrent calls)
# These are conservative estimates based on known provider limits
PROVIDER_RATE_LIMITS: dict[ProviderType, dict[str, int]] = {
    ProviderType.GOOGLE: {
        "rpm": 60,              # Requests per minute
        "max_concurrent": 8,    # Safe concurrent calls
    },
    ProviderType.OPENROUTER: {
        "rpm": 30,              # Conservative default (varies by model)
        "max_concurrent": 5,    # Safe concurrent calls
    },
}

# Tier-based concurrent limits for each parallelism mode
# ModelTier is defined in llm_router.py, but we reference by string here
TIER_CONCURRENT_LIMITS: dict[str, dict[str, int]] = {
    "free": {
        "sequential": 1,
        "normal": 1,
        "aggressive": 2,
        "max": 2,       # Free models have strict rate limits
    },
    "paid": {
        "sequential": 1,
        "normal": 3,
        "aggressive": 5,
        "max": 6,
    },
    "native": {
        "sequential": 1,
        "normal": 3,
        "aggressive": 5,
        "max": 8,       # Google native has generous limits
    },
}


def get_preset_parallelism(preset: QualityPreset) -> ParallelismMode:
    """Get the parallelism mode for a preset.

    Args:
        preset: The quality preset.

    Returns:
        ParallelismMode for the preset.
    """
    return PRESET_PARALLELISM.get(preset, ParallelismMode.NORMAL)


def get_tier_max_concurrent(tier: str, mode: ParallelismMode) -> int:
    """Get maximum concurrent calls for a tier and parallelism mode.

    Args:
        tier: Model tier ('free', 'paid', 'native')
        mode: Parallelism mode

    Returns:
        Maximum concurrent calls allowed
    """
    tier_limits = TIER_CONCURRENT_LIMITS.get(tier, TIER_CONCURRENT_LIMITS["paid"])
    return tier_limits.get(mode.value, tier_limits["normal"])


class Settings(BaseSettings):
    """Application settings with provider configuration.

    Settings are loaded from environment variables and .env file.
    At least one provider API key (GOOGLE_API_KEY or OPENROUTER_API_KEY) is required.

    Attributes:
        DATABASE_URL: Database connection string (SQLite or PostgreSQL)
        GOOGLE_API_KEY: Google AI API key for Gemini models
        OPENROUTER_API_KEY: OpenRouter API key for multi-model access
        PRIMARY_PROVIDER: Primary LLM provider (google or openrouter)
        FALLBACK_PROVIDER: Fallback provider when primary fails
        JUDGE_MODEL: Model for fast validation/judging tasks
        CREATIVE_MODEL: Model for complex creative generation
        IMAGE_MODEL: Model for image generation
    """

    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=True,
        extra="ignore",
    )

    # Database
    DATABASE_URL: str = Field(
        default="sqlite+aiosqlite:///./timepoint.db",
        description="Database connection string",
    )

    # Provider API Keys
    GOOGLE_API_KEY: str | None = Field(
        default=None,
        description="Google AI API key",
    )
    OPENROUTER_API_KEY: str | None = Field(
        default=None,
        description="OpenRouter API key",
    )

    # Provider Selection
    PRIMARY_PROVIDER: ProviderType = Field(
        default=ProviderType.GOOGLE,
        description="Primary LLM provider",
    )
    FALLBACK_PROVIDER: ProviderType | None = Field(
        default=ProviderType.OPENROUTER,
        description="Fallback provider when primary fails",
    )

    # Model Selection
    JUDGE_MODEL: str = Field(
        default="gemini-2.5-flash",
        description="Model for validation/judging (fast)",
    )
    CREATIVE_MODEL: str = Field(
        default="gemini-2.5-flash",  # VerifiedModels.GOOGLE_TEXT[0]
        description="Model for creative generation (quality)",
    )
    IMAGE_MODEL: str = Field(
        default="gemini-2.5-flash-image",
        description="Model for image generation (via native Google)",
    )

    # Observability
    LOGFIRE_TOKEN: str | None = Field(
        default=None,
        description="Logfire token for monitoring",
    )

    # Application Settings
    ENVIRONMENT: Environment = Field(
        default=Environment.DEVELOPMENT,
        description="Application environment",
    )
    DEBUG: bool = Field(
        default=True,
        description="Enable debug mode",
    )
    RATE_LIMIT: int = Field(
        default=60,
        description="API rate limit (requests per minute)",
    )

    # Pipeline Settings
    PIPELINE_MAX_PARALLELISM: int = Field(
        default=3,
        description="Maximum parallel LLM calls in pipeline (1-5)",
        ge=1,
        le=5,
    )

    # Blob Storage
    BLOB_STORAGE_ENABLED: bool = Field(
        default=False,
        description="Enable blob storage for timepoint assets",
    )
    BLOB_STORAGE_ROOT: str = Field(
        default="./output/timepoints",
        description="Root directory for blob storage output",
    )

    @field_validator("DATABASE_URL")
    @classmethod
    def validate_database_url(cls, v: str) -> str:
        """Validate database URL format."""
        valid_prefixes = ("sqlite", "postgresql", "postgres")
        if not any(v.startswith(prefix) for prefix in valid_prefixes):
            raise ValueError(
                f"DATABASE_URL must start with one of: {valid_prefixes}"
            )
        return v

    @model_validator(mode="after")
    def validate_providers(self) -> "Settings":
        """Check provider API key configuration (soft validation).

        Note: This no longer raises an error to allow the app to start
        without API keys configured. The health endpoint will report
        providers as unavailable instead.
        """
        # Soft validation - just track if any providers are available
        # The app will start but providers will be marked as unavailable
        self._has_any_provider = bool(self.GOOGLE_API_KEY or self.OPENROUTER_API_KEY)
        return self

    @property
    def has_any_provider(self) -> bool:
        """Check if any provider API key is configured."""
        return bool(self.GOOGLE_API_KEY or self.OPENROUTER_API_KEY)

    @property
    def is_production(self) -> bool:
        """Check if running in production environment."""
        return self.ENVIRONMENT == Environment.PRODUCTION

    @property
    def is_sqlite(self) -> bool:
        """Check if using SQLite database."""
        return self.DATABASE_URL.startswith("sqlite")

    @property
    def detected_provider(self) -> ProviderType:
        """Auto-detect primary provider based on available API keys.

        Returns:
            ProviderType: The detected provider based on API key availability.

        Raises:
            ValueError: If no API keys are configured.
        """
        if self.GOOGLE_API_KEY:
            return ProviderType.GOOGLE
        elif self.OPENROUTER_API_KEY:
            return ProviderType.OPENROUTER
        raise ValueError("No API keys configured")

    def has_provider(self, provider: ProviderType) -> bool:
        """Check if a specific provider is configured.

        Args:
            provider: The provider to check.

        Returns:
            bool: True if the provider's API key is configured.
        """
        if provider == ProviderType.GOOGLE:
            return bool(self.GOOGLE_API_KEY)
        elif provider == ProviderType.OPENROUTER:
            return bool(self.OPENROUTER_API_KEY)
        return False

    def get_api_key(self, provider: ProviderType) -> str:
        """Get API key for a specific provider.

        Args:
            provider: The provider to get the key for.

        Returns:
            str: The API key.

        Raises:
            ValueError: If the provider's API key is not configured.
        """
        if provider == ProviderType.GOOGLE:
            if not self.GOOGLE_API_KEY:
                raise ValueError("GOOGLE_API_KEY not configured")
            return self.GOOGLE_API_KEY
        elif provider == ProviderType.OPENROUTER:
            if not self.OPENROUTER_API_KEY:
                raise ValueError("OPENROUTER_API_KEY not configured")
            return self.OPENROUTER_API_KEY
        raise ValueError(f"Unknown provider: {provider}")

    def get_model_config(self) -> dict[str, Any]:
        """Get model configuration dictionary.

        Returns:
            dict: Model configuration with judge, creative, and image models.
        """
        return {
            "judge": self.JUDGE_MODEL,
            "creative": self.CREATIVE_MODEL,
            "image": self.IMAGE_MODEL,
        }

    def get_preset_config(self, preset: QualityPreset) -> dict[str, Any]:
        """Get configuration for a quality preset.

        Args:
            preset: The quality preset.

        Returns:
            dict: Preset configuration with models, providers, and settings.
        """
        return PRESET_CONFIGS[preset]


@lru_cache
def get_settings() -> Settings:
    """Get cached settings instance.

    Returns:
        Settings: The application settings.

    Examples:
        >>> settings = get_settings()
        >>> settings.PRIMARY_PROVIDER
        <ProviderType.GOOGLE: 'google'>
    """
    return Settings()


# Global settings instance for convenience
settings = get_settings()


def validate_presets() -> list[str]:
    """Validate that all presets use only verified models.

    This function should be called at startup to catch configuration errors
    early. It checks that all models in PRESET_CONFIGS are in VerifiedModels.

    Returns:
        list[str]: List of validation errors (empty if all valid).

    Raises:
        ValueError: If raise_on_error=True and validation fails.

    Examples:
        >>> errors = validate_presets()
        >>> if errors:
        ...     print("Configuration errors:", errors)
    """
    errors = []

    for preset, config in PRESET_CONFIGS.items():
        text_model = config.get("text_model", "")
        judge_model = config.get("judge_model", "")
        image_model = config.get("image_model", "")
        text_provider = config.get("text_provider", ProviderType.GOOGLE)

        # Validate text model
        if text_provider == ProviderType.GOOGLE:
            if text_model not in VerifiedModels.GOOGLE_TEXT:
                errors.append(
                    f"{preset.value}: text_model '{text_model}' not in VerifiedModels.GOOGLE_TEXT"
                )
        else:
            if text_model not in VerifiedModels.OPENROUTER_TEXT:
                errors.append(
                    f"{preset.value}: text_model '{text_model}' not in VerifiedModels.OPENROUTER_TEXT"
                )

        # Validate judge model (follows text provider)
        if text_provider == ProviderType.GOOGLE:
            if judge_model not in VerifiedModels.GOOGLE_TEXT:
                errors.append(
                    f"{preset.value}: judge_model '{judge_model}' not in VerifiedModels.GOOGLE_TEXT"
                )
        else:
            if judge_model not in VerifiedModels.OPENROUTER_TEXT:
                errors.append(
                    f"{preset.value}: judge_model '{judge_model}' not in VerifiedModels.OPENROUTER_TEXT"
                )

        # Validate image model
        if image_model and image_model not in VerifiedModels.GOOGLE_IMAGE:
            errors.append(
                f"{preset.value}: image_model '{image_model}' not in VerifiedModels.GOOGLE_IMAGE"
            )

    return errors


def validate_presets_or_raise() -> None:
    """Validate presets and raise if any errors found.

    Raises:
        ValueError: If any preset uses unverified models.
    """
    errors = validate_presets()
    if errors:
        raise ValueError(
            f"Preset configuration errors (models not in VerifiedModels):\n"
            + "\n".join(f"  - {e}" for e in errors)
        )
