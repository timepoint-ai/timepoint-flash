# TIMEPOINT Flash v2.3.0 Environment Configuration
# Copy this file to .env and update values

# =============================================================================
# Database Configuration
# =============================================================================

# SQLite (development - default)
DATABASE_URL=sqlite+aiosqlite:///./timepoint.db

# PostgreSQL (production)
# DATABASE_URL=postgresql+asyncpg://user:password@localhost:5432/timepoint

# =============================================================================
# Provider API Keys (at least one required)
# =============================================================================

# Google AI (Gemini models)
GOOGLE_API_KEY=your-google-api-key-here

# OpenRouter (multi-model access)
# OPENROUTER_API_KEY=your-openrouter-api-key-here

# =============================================================================
# Provider Configuration
# =============================================================================

# Primary provider: google or openrouter
PRIMARY_PROVIDER=google

# Fallback provider (optional)
FALLBACK_PROVIDER=openrouter

# =============================================================================
# Model Selection
# =============================================================================

# Fast model for validation/judging
JUDGE_MODEL=gemini-2.5-flash

# Quality model for creative generation
CREATIVE_MODEL=gemini-3-pro-preview

# Image generation model
IMAGE_MODEL=google/gemini-3-pro-image-preview

# =============================================================================
# Application Settings
# =============================================================================

# Environment: development, staging, production
ENVIRONMENT=development

# Debug mode (set to false in production)
DEBUG=true

# Rate limit (requests per minute)
RATE_LIMIT=60

# =============================================================================
# Pipeline Settings
# =============================================================================

# Maximum parallel LLM calls during generation (1-5)
# Higher values = faster generation but more API calls at once
# Default: 3 (graph, moment, camera steps run in parallel)
PIPELINE_MAX_PARALLELISM=3

# =============================================================================
# Observability (Optional)
# =============================================================================

# Logfire monitoring token
# LOGFIRE_TOKEN=your-logfire-token
